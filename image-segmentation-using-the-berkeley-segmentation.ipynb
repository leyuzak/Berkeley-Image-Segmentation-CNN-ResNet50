{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1553396,"sourceType":"datasetVersion","datasetId":916809}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Segmentation Using the Berkeley Segmentation Dataset (BSDS)","metadata":{}},{"cell_type":"markdown","source":"<img src= \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQorh8yqNAjHPP7JurjNB3xgn5-aVmypC4t-A&s\" width=600>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## ðŸŽ¯ Project Objective\n\nThis project aims to design, implement, and evaluate deep learningâ€“based image segmentation models using the **Berkeley Segmentation Dataset (BSDS500)**.\n\nThe main objectives of the project are:\n\n- To build a **Baseline Convolutional Neural Network (CNN)** for pixel-level boundary segmentation without using transfer learning.  \n- To develop an **Advanced Segmentation Model using Transfer Learning with ResNet50**, including fine-tuning.  \n- To preprocess imageâ€“mask pairs and convert raw human annotations into usable binary segmentation masks.  \n- To compare the performance of the baseline CNN and the transfer learning model.  \n- To deploy trained models using **Streamlit applications** for interactive inference.\n\nThis project focuses on understanding the **end-to-end image segmentation pipeline**, from data preprocessing to model evaluation and deployment.\n","metadata":{}},{"cell_type":"code","source":"print(\"session started\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:39:36.297178Z","iopub.execute_input":"2025-12-21T13:39:36.297583Z","iopub.status.idle":"2025-12-21T13:39:36.309533Z","shell.execute_reply.started":"2025-12-21T13:39:36.297544Z","shell.execute_reply":"2025-12-21T13:39:36.308378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport scipy.io as sio\nimport pandas as pd\nfrom PIL import Image\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import UpSampling2D\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import MeanIoU\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.layers import Dropout","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:15:47.661821Z","iopub.execute_input":"2025-12-21T13:15:47.662532Z","iopub.status.idle":"2025-12-21T13:16:01.722235Z","shell.execute_reply.started":"2025-12-21T13:15:47.662498Z","shell.execute_reply":"2025-12-21T13:16:01.721480Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tf.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:01.723507Z","iopub.execute_input":"2025-12-21T13:16:01.724134Z","iopub.status.idle":"2025-12-21T13:16:01.728676Z","shell.execute_reply.started":"2025-12-21T13:16:01.724103Z","shell.execute_reply":"2025-12-21T13:16:01.727447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_path = \"/kaggle/input/berkeley-segmentation-dataset-500-bsds500/images/\"\ngt_path  = \"/kaggle/input/berkeley-segmentation-dataset-500-bsds500/ground_truth/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:01.729599Z","iopub.execute_input":"2025-12-21T13:16:01.729872Z","iopub.status.idle":"2025-12-21T13:16:01.844141Z","shell.execute_reply.started":"2025-12-21T13:16:01.729840Z","shell.execute_reply":"2025-12-21T13:16:01.843414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.listdir(img_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:01.845634Z","iopub.execute_input":"2025-12-21T13:16:01.845857Z","iopub.status.idle":"2025-12-21T13:16:01.871971Z","shell.execute_reply.started":"2025-12-21T13:16:01.845838Z","shell.execute_reply":"2025-12-21T13:16:01.871269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.listdir(gt_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:01.872886Z","iopub.execute_input":"2025-12-21T13:16:01.873284Z","iopub.status.idle":"2025-12-21T13:16:01.887557Z","shell.execute_reply.started":"2025-12-21T13:16:01.873214Z","shell.execute_reply":"2025-12-21T13:16:01.887036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = [\"train\", \"val\", \"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:01.888194Z","iopub.execute_input":"2025-12-21T13:16:01.888760Z","iopub.status.idle":"2025-12-21T13:16:01.891778Z","shell.execute_reply.started":"2025-12-21T13:16:01.888739Z","shell.execute_reply":"2025-12-21T13:16:01.891185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_list = []\nmask_list = []\nlabel_list = []\n\nfor label in labels:\n    img_dir = os.path.join(img_path, label)\n    gt_dir = os.path.join(gt_path, label)\n\n    for img_file in os.listdir(img_dir):\n        if img_file.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n            img_full_path = os.path.join(img_dir, img_file)\n            name, _ = os.path.splitext(img_file)\n            mask_file = name + \".mat\"\n            mask_full_path = os.path.join(gt_dir, mask_file)\n\n            img_list.append(img_full_path)\n            mask_list.append(mask_full_path)\n            label_list.append(label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:01.892729Z","iopub.execute_input":"2025-12-21T13:16:01.893050Z","iopub.status.idle":"2025-12-21T13:16:01.939184Z","shell.execute_reply.started":"2025-12-21T13:16:01.893028Z","shell.execute_reply":"2025-12-21T13:16:01.938697Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ“ Dataset Description\n\nThe **Berkeley Segmentation Dataset 500 (BSDS500)** is a widely used benchmark dataset for image segmentation and boundary detection tasks.\n\nDataset characteristics:\n\n- Total number of images: **500**  \n- Training set: **200 images**  \n- Validation set: **100 images**  \n- Test set: **200 images**\n\nEach image is annotated by multiple human annotators.  \nGround-truth annotations are provided in `.mat` files containing boundary maps for each annotator.\n\nFor this project, multiple boundary annotations per image are combined into a **single binary boundary mask**.","metadata":{"execution":{"iopub.status.busy":"2025-12-20T13:15:34.930911Z","iopub.execute_input":"2025-12-20T13:15:34.931876Z","iopub.status.idle":"2025-12-20T13:15:34.937312Z","shell.execute_reply.started":"2025-12-20T13:15:34.931837Z","shell.execute_reply":"2025-12-20T13:15:34.936396Z"}}},{"cell_type":"code","source":"df = pd.DataFrame({\"img\": img_list,\"mask\": mask_list,\"label\": label_list})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:01.939921Z","iopub.execute_input":"2025-12-21T13:16:01.940152Z","iopub.status.idle":"2025-12-21T13:16:01.947120Z","shell.execute_reply.started":"2025-12-21T13:16:01.940120Z","shell.execute_reply":"2025-12-21T13:16:01.946532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:01.947888Z","iopub.execute_input":"2025-12-21T13:16:01.948135Z","iopub.status.idle":"2025-12-21T13:16:01.960657Z","shell.execute_reply.started":"2025-12-21T13:16:01.948117Z","shell.execute_reply":"2025-12-21T13:16:01.959973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"label\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:01.962941Z","iopub.execute_input":"2025-12-21T13:16:01.963142Z","iopub.status.idle":"2025-12-21T13:16:01.983845Z","shell.execute_reply.started":"2025-12-21T13:16:01.963124Z","shell.execute_reply":"2025-12-21T13:16:01.983307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(cv2.cvtColor(cv2.imread(df.iloc[0][\"img\"]), cv2.COLOR_BGR2RGB))\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:01.984589Z","iopub.execute_input":"2025-12-21T13:16:01.984799Z","iopub.status.idle":"2025-12-21T13:16:02.173223Z","shell.execute_reply.started":"2025-12-21T13:16:01.984780Z","shell.execute_reply":"2025-12-21T13:16:02.172578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:02.174158Z","iopub.execute_input":"2025-12-21T13:16:02.174524Z","iopub.status.idle":"2025-12-21T13:16:02.190655Z","shell.execute_reply.started":"2025-12-21T13:16:02.174491Z","shell.execute_reply":"2025-12-21T13:16:02.190080Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:02.191478Z","iopub.execute_input":"2025-12-21T13:16:02.191804Z","iopub.status.idle":"2025-12-21T13:16:02.200179Z","shell.execute_reply.started":"2025-12-21T13:16:02.191782Z","shell.execute_reply":"2025-12-21T13:16:02.199573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"d_map = {\"train\": 0,\"val\": 1,\"test\": 2}\ndf[\"encode_label\"] = df[\"label\"].map(d_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:02.201041Z","iopub.execute_input":"2025-12-21T13:16:02.201297Z","iopub.status.idle":"2025-12-21T13:16:02.217304Z","shell.execute_reply.started":"2025-12-21T13:16:02.201248Z","shell.execute_reply":"2025-12-21T13:16:02.216657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:02.218161Z","iopub.execute_input":"2025-12-21T13:16:02.218488Z","iopub.status.idle":"2025-12-21T13:16:02.233309Z","shell.execute_reply.started":"2025-12-21T13:16:02.218459Z","shell.execute_reply":"2025-12-21T13:16:02.232623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images = []\n\nfor img_path in img_list[:3]:\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    images.append(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:02.234066Z","iopub.execute_input":"2025-12-21T13:16:02.234237Z","iopub.status.idle":"2025-12-21T13:16:02.266097Z","shell.execute_reply.started":"2025-12-21T13:16:02.234221Z","shell.execute_reply":"2025-12-21T13:16:02.265627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8, 3))\n\nfor i, image in enumerate(images):\n    plt.subplot(1, 3, i + 1)\n    plt.imshow(image)\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:02.266869Z","iopub.execute_input":"2025-12-21T13:16:02.267083Z","iopub.status.idle":"2025-12-21T13:16:02.406435Z","shell.execute_reply.started":"2025-12-21T13:16:02.267065Z","shell.execute_reply":"2025-12-21T13:16:02.405766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_SIZE = (168, 168)\n\nx = []\nfor img_path in df[\"img\"]:\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, IMG_SIZE)\n    img = img.astype(np.float32) / 255.0\n    x.append(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:02.407249Z","iopub.execute_input":"2025-12-21T13:16:02.407552Z","iopub.status.idle":"2025-12-21T13:16:08.159835Z","shell.execute_reply.started":"2025-12-21T13:16:02.407525Z","shell.execute_reply":"2025-12-21T13:16:08.159067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = np.array(x, dtype=np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:08.160801Z","iopub.execute_input":"2025-12-21T13:16:08.161300Z","iopub.status.idle":"2025-12-21T13:16:08.215280Z","shell.execute_reply.started":"2025-12-21T13:16:08.161235Z","shell.execute_reply":"2025-12-21T13:16:08.214692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y = []\n\nfor mask_path in df[\"mask\"]:\n    mat = sio.loadmat(mask_path)\n    gts = mat[\"groundTruth\"][0]\n    edges = []\n    for g in gts:\n        edges.append(g[0][0][1])  \n    mask = np.max(edges, axis=0)\n    mask = cv2.resize(mask, (168, 168), interpolation=cv2.INTER_NEAREST)\n    mask = (mask > 0).astype(np.float32)\n    mask = np.expand_dims(mask, axis=-1)\n    y.append(mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:08.216108Z","iopub.execute_input":"2025-12-21T13:16:08.216389Z","iopub.status.idle":"2025-12-21T13:16:14.855717Z","shell.execute_reply.started":"2025-12-21T13:16:08.216366Z","shell.execute_reply":"2025-12-21T13:16:14.855096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y = np.array(y, dtype=np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:14.856542Z","iopub.execute_input":"2025-12-21T13:16:14.856829Z","iopub.status.idle":"2025-12-21T13:16:14.873191Z","shell.execute_reply.started":"2025-12-21T13:16:14.856796Z","shell.execute_reply":"2025-12-21T13:16:14.872666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = df[df[\"label\"] == \"train\"].reset_index(drop=True)\ndf_val   = df[df[\"label\"] == \"val\"].reset_index(drop=True)\ndf_test  = df[df[\"label\"] == \"test\"].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:14.874154Z","iopub.execute_input":"2025-12-21T13:16:14.874807Z","iopub.status.idle":"2025-12-21T13:16:14.880603Z","shell.execute_reply.started":"2025-12-21T13:16:14.874784Z","shell.execute_reply":"2025-12-21T13:16:14.880014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_idx = df.index[df[\"label\"] == \"train\"].to_numpy()\nval_idx   = df.index[df[\"label\"] == \"val\"].to_numpy()\ntest_idx  = df.index[df[\"label\"] == \"test\"].to_numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:14.881502Z","iopub.execute_input":"2025-12-21T13:16:14.881747Z","iopub.status.idle":"2025-12-21T13:16:14.893682Z","shell.execute_reply.started":"2025-12-21T13:16:14.881718Z","shell.execute_reply":"2025-12-21T13:16:14.893110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train, y_train = x[train_idx], y[train_idx]\nx_val, y_val     = x[val_idx], y[val_idx]\nx_test, y_test   = x[test_idx], y[test_idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:14.894470Z","iopub.execute_input":"2025-12-21T13:16:14.895031Z","iopub.status.idle":"2025-12-21T13:16:14.969853Z","shell.execute_reply.started":"2025-12-21T13:16:14.895011Z","shell.execute_reply":"2025-12-21T13:16:14.969079Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ§  Baseline Model: Custom CNN\n\nA custom **Encoderâ€“Decoder Convolutional Neural Network (CNN)** is implemented as the baseline segmentation model.\n\nKey characteristics of the baseline CNN:\n\n- Sequential encoderâ€“decoder architecture.  \n- Convolutional layers followed by Batch Normalization and Max Pooling for feature extraction.  \n- Upsampling layers for spatial resolution recovery.  \n- Sigmoid activation function for binary segmentation output.  \n- Binary Cross-Entropy loss function.  \n- Regularization using **Early Stopping** and **ReduceLROnPlateau** callbacks.\n\nThis model is trained **from scratch**, without any pretrained weights, and serves as a baseline for comparison.","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Input(shape=(168, 168, 3)))\nmodel.add(Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.1))\nmodel.add(Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.3))\nmodel.add(Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Conv2D(1, (1, 1), activation=\"sigmoid\", padding=\"same\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:16:14.970831Z","iopub.execute_input":"2025-12-21T13:16:14.971227Z","iopub.status.idle":"2025-12-21T13:16:17.598146Z","shell.execute_reply.started":"2025-12-21T13:16:14.971194Z","shell.execute_reply":"2025-12-21T13:16:17.597578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dice_loss(y_true, y_pred, smooth=1.0):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n\n    y_true_f = tf.reshape(y_true, [tf.shape(y_true)[0], -1])\n    y_pred_f = tf.reshape(y_pred, [tf.shape(y_pred)[0], -1])\n\n    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=1)\n    union = tf.reduce_sum(y_true_f, axis=1) + tf.reduce_sum(y_pred_f, axis=1)\n\n    dice = (2.0 * intersection + smooth) / (union + smooth)\n    return 1.0 - dice  \n\ndef bce_dice_loss(y_true, y_pred):\n    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n    bce = tf.reduce_mean(bce, axis=[1, 2])                     \n    d = dice_loss(y_true, y_pred)                              \n    return tf.reduce_mean(bce + d) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:17:29.326935Z","iopub.execute_input":"2025-12-21T13:17:29.327732Z","iopub.status.idle":"2025-12-21T13:17:29.333231Z","shell.execute_reply.started":"2025-12-21T13:17:29.327699Z","shell.execute_reply":"2025-12-21T13:17:29.332643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n    optimizer=\"adam\",\n    loss=bce_dice_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:17:40.490489Z","iopub.execute_input":"2025-12-21T13:17:40.491039Z","iopub.status.idle":"2025-12-21T13:17:40.498617Z","shell.execute_reply.started":"2025-12-21T13:17:40.491011Z","shell.execute_reply":"2025-12-21T13:17:40.497982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"es = EarlyStopping(\n    monitor=\"val_loss\",\n    patience=3,\n    restore_best_weights=True)\n\nrlr = ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=2,\n    min_lr=1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:18:14.062948Z","iopub.execute_input":"2025-12-21T13:18:14.063247Z","iopub.status.idle":"2025-12-21T13:18:14.067236Z","shell.execute_reply.started":"2025-12-21T13:18:14.063222Z","shell.execute_reply":"2025-12-21T13:18:14.066626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    x_train, y_train,\n    validation_data=(x_val, y_val),\n    epochs=30,\n    batch_size=8,\n    callbacks=[es, rlr])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:18:17.464337Z","iopub.execute_input":"2025-12-21T13:18:17.464639Z","iopub.status.idle":"2025-12-21T13:20:24.454858Z","shell.execute_reply.started":"2025-12-21T13:18:17.464613Z","shell.execute_reply":"2025-12-21T13:20:24.454031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred = model.predict(x_val[:5])\n\nplt.figure(figsize=(12, 6))\nfor i in range(5):\n    plt.subplot(5, 3, 3*i+1)\n    plt.imshow(x_val[i])\n    plt.title(\"Image\")\n    plt.axis(\"off\")\n\n    plt.subplot(5, 3, 3*i+2)\n    plt.imshow(y_val[i].squeeze(), cmap=\"gray\")\n    plt.title(\"GT\")\n    plt.axis(\"off\")\n\n    plt.subplot(5, 3, 3*i+3)\n    plt.imshow((pred[i].squeeze() > 0.10), cmap=\"gray\")\n    plt.title(\"Pred (t=0.10)\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:27:48.651197Z","iopub.execute_input":"2025-12-21T13:27:48.651970Z","iopub.status.idle":"2025-12-21T13:27:49.236382Z","shell.execute_reply.started":"2025-12-21T13:27:48.651942Z","shell.execute_reply":"2025-12-21T13:27:49.235757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:21:04.681158Z","iopub.execute_input":"2025-12-21T13:21:04.681844Z","iopub.status.idle":"2025-12-21T13:21:04.724703Z","shell.execute_reply.started":"2025-12-21T13:21:04.681815Z","shell.execute_reply":"2025-12-21T13:21:04.724076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate(x_test, y_test, batch_size=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:21:08.998488Z","iopub.execute_input":"2025-12-21T13:21:08.998794Z","iopub.status.idle":"2025-12-21T13:21:10.842136Z","shell.execute_reply.started":"2025-12-21T13:21:08.998768Z","shell.execute_reply":"2025-12-21T13:21:10.841602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"cnn_model.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:27:57.101695Z","iopub.execute_input":"2025-12-21T13:27:57.102233Z","iopub.status.idle":"2025-12-21T13:27:57.533126Z","shell.execute_reply.started":"2025-12-21T13:27:57.102207Z","shell.execute_reply":"2025-12-21T13:27:57.532588Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸš€ Transfer Learning with ResNet50\n\nFor the advanced model, **ResNet50 pretrained on ImageNet** is used as the encoder backbone.\n\nTransfer learning strategy:\n\n- A pretrained ResNet50 network is used with `include_top=False`.  \n- The backbone is initially **frozen** to preserve pretrained feature representations.  \n- A custom CNN-based decoder with upsampling layers is added on top of the encoder.  \n- After initial training, **fine-tuning** is performed by unfreezing the last layers of ResNet50.  \n- Binary Cross-Entropy loss is used for segmentation training.\n\nThis approach leverages pretrained semantic features to improve segmentation performance compared to the baseline CNN.","metadata":{}},{"cell_type":"code","source":"IMG_SIZE = (224, 224)\n\nx224 = []\nfor img_path in df[\"img\"]:\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, IMG_SIZE)\n    img = img.astype(np.float32)\n    img = preprocess_input(img)         \n    x224.append(img)\n\nx224 = np.array(x224, dtype=np.float32)\n\n\ny224 = []\nfor mask_path in df[\"mask\"]:\n    mat = sio.loadmat(mask_path)\n    gts = mat[\"groundTruth\"][0]\n\n    edges = []\n    for g in gts:\n        edges.append(g[0][0][1])\n\n    mask = np.max(edges, axis=0)\n\n    mask = cv2.resize(mask, IMG_SIZE, interpolation=cv2.INTER_NEAREST)\n    mask = (mask > 0).astype(np.float32)                              \n    mask = np.expand_dims(mask, axis=-1)\n\n    y224.append(mask)\n\ny224 = np.array(y224, dtype=np.float32)\n\n\nx224_train, y224_train = x224[train_idx], y224[train_idx]\nx224_val,   y224_val   = x224[val_idx],   y224[val_idx]\nx224_test,  y224_test  = x224[test_idx],  y224[test_idx]\n\nprint(\"Train:\", x224_train.shape, y224_train.shape)\nprint(\"Val  :\", x224_val.shape, y224_val.shape)\nprint(\"Test :\", x224_test.shape, y224_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:27:59.842992Z","iopub.execute_input":"2025-12-21T13:27:59.843839Z","iopub.status.idle":"2025-12-21T13:28:05.742731Z","shell.execute_reply.started":"2025-12-21T13:27:59.843809Z","shell.execute_reply":"2025-12-21T13:28:05.741888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model = ResNet50(\n    weights=\"imagenet\",\n    include_top=False,\n    input_shape=(224, 224, 3))\n\ninputs = layers.Input(shape=(224, 224, 3))\nz = base_model(inputs, training=False)  \n\nz = layers.UpSampling2D((2, 2))(z)    \nz = layers.Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(z)\nz = layers.BatchNormalization()(z)\n\nz = layers.UpSampling2D((2, 2))(z)     \nz = layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(z)\nz = layers.BatchNormalization()(z)\n\nz = layers.UpSampling2D((2, 2))(z)      \nz = layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(z)\nz = layers.BatchNormalization()(z)\n\nz = layers.UpSampling2D((2, 2))(z)  \nz = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(z)\nz = layers.BatchNormalization()(z)\n\nz = layers.UpSampling2D((2, 2))(z)   \noutputs = layers.Conv2D(1, (1, 1), activation=\"sigmoid\", padding=\"same\")(z)\n\nmodel_tl = models.Model(inputs, outputs)\n\nearly_stop = EarlyStopping(\n    monitor=\"val_loss\",\n    patience=5,\n    restore_best_weights=True)\n\nlr_reduce = ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.2,\n    patience=2,\n    min_lr=1e-6)\n\nbase_model.trainable = False\n\nmodel_tl.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-3),\n    loss=\"binary_crossentropy\")\n\nhistory = model_tl.fit(\n    x224_train, y224_train,\n    validation_data=(x224_val, y224_val),\n    epochs=15,\n    batch_size=8,\n    callbacks=[early_stop, lr_reduce])\n\nbase_model.trainable = True\nfor layer in base_model.layers[:-30]:\n    layer.trainable = False\n\nmodel_tl.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-5),\n    loss=\"binary_crossentropy\")\n\nhistory_ft = model_tl.fit(\n    x224_train, y224_train,\n    validation_data=(x224_val, y224_val),\n    epochs=10,\n    batch_size=8,\n    callbacks=[early_stop, lr_reduce])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:28:13.766695Z","iopub.execute_input":"2025-12-21T13:28:13.767002Z","iopub.status.idle":"2025-12-21T13:30:04.708203Z","shell.execute_reply.started":"2025-12-21T13:28:13.766977Z","shell.execute_reply":"2025-12-21T13:30:04.707597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_tl.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:30:04.709912Z","iopub.execute_input":"2025-12-21T13:30:04.710510Z","iopub.status.idle":"2025-12-21T13:30:04.736910Z","shell.execute_reply.started":"2025-12-21T13:30:04.710488Z","shell.execute_reply":"2025-12-21T13:30:04.736083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_tl.save(\"resnet50_model.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:32:51.604808Z","iopub.execute_input":"2025-12-21T13:32:51.605126Z","iopub.status.idle":"2025-12-21T13:32:52.898500Z","shell.execute_reply.started":"2025-12-21T13:32:51.605100Z","shell.execute_reply":"2025-12-21T13:32:52.897491Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ“Š Results Comparison\n\n### Quantitative Results (Test Set)\n\n| Model | Input Size | Loss Function | Best Threshold | Validation IoU | Test IoU (Approx.) | Notes |\n|------|-----------|---------------|----------------|----------------|-------------------|-------|\n| Baseline CNN | 168 Ã— 168 | BCE + Dice | **0.10** | **â‰ˆ 0.50** | â‰ˆ 0.49â€“0.50 | Produces sharper boundary predictions |\n| ResNet50 (Transfer Learning) | 224 Ã— 224 | BCE | **0.15 â€“ 0.20** | â‰ˆ 0.48 | â‰ˆ 0.47â€“0.48 | Strong encoder, limited decoder capacity |\n\n---\n\n### Qualitative Comparison\n\n| Aspect | Baseline CNN | ResNet50 |\n|------|-------------|----------|\n| Boundary continuity | Better | Slightly fragmented |\n| Thin edge detection | Better | Weaker |\n| Over-smoothing | Low | Moderate |\n| Visual similarity to GT | Higher | Moderate |\n\n---\n\n## ðŸ§  Project Summary\n\nThis project investigates **image boundary segmentation** on the **Berkeley Segmentation Dataset** using two deep learning approaches: a **custom baseline CNN** and a **ResNet50-based transfer learning model**.\n\nThe baseline CNN was designed as an encoderâ€“decoder architecture trained from scratch and optimized using a **Binary Cross-Entropy (BCE) + Dice loss** to mitigate the severe class imbalance present in boundary segmentation tasks. The ResNet50 model employed a pretrained ImageNet encoder followed by a lightweight upsampling-based decoder.\n\nModel performance was evaluated using the **Intersection over Union (IoU)** metric and qualitative visual comparisons of predicted boundary masks. Threshold tuning was applied to each model to determine the optimal binarization value.\n\n### Key Findings\n- The baseline CNN achieved the **highest performance**, reaching a validation IoU of approximately **0.50** at a threshold of **0.10**.\n- The ResNet50-based model achieved a slightly lower validation IoU of approximately **0.48**, despite leveraging pretrained features.\n- Qualitative evaluation showed that the baseline CNN produced **more continuous and accurate boundary predictions**, particularly for thin edges.\n- The limited improvement of the ResNet model is attributed to the absence of **skip connections**, which are essential for preserving fine-grained spatial details in segmentation tasks.\n\n### Conclusion\nThe results indicate that for **edge-based image segmentation**, a well-designed CNN trained from scratch with an appropriate loss function can outperform a deeper transfer learning model. Future work may explore **UNet-style architectures with skip connections** to further improve segmentation accuracy.\n","metadata":{}}]}